# Relationship Extraction Assignment

This is a quick "proof of concept" model, based on bi-directional LSTM with attention. The idea of the implementation of attention mechanism is based on P.Zhou et al paper <a href=http://www.aclweb.org/anthology/P16-2034>Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification</a>. I choose this simple architecture after some brief literature research as the most feasible implementation given time constraints, and also because I already had BiLSTM building blocks from my earlier project. In addition, some implementation inspirations were taken from paper re-implementation <a href=https://github.com/SeoSangwoo/Attention-Based-BiLSTM-relation-extraction>github repo</a> (data transform, attention details).<br>
Honestly, this is quite a raw try (due to time constraints), with practically out-of-the-box model parameters. The model clearly overfits (for the sake of time I've skipped extra L2 regularisation used in repo), but nevertheless it reaches F1 around 0.68 (on the offical test set) practically without parameters adjustments. Although shy of 0.84 reported in the paper for this architecture, with some tweaks and polishes I believe the score would improve.<br>

To run the notebook only the paths and the file names should be adjusted in the *Initialising parameters* section (and 2(3) data files copied correspondingly). For the docker container, mount local volume need to **/tf** path inside the container, jupyter-notebook port is specified as **8080**. GPU support is used, but will work without as well.
