{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship Extraction\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick \"proof of concept\" model, based on bi-directional LSTM with attention. The idea of the implementation of attention mechanism is based on P.Zhou et al paper <a href=http://www.aclweb.org/anthology/P16-2034>Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification</a>. I choose this simple architecture after some brief literature research as the most feasible implementation given time constraints, and also because I already had BiLSTM building blocks from my earlier project. In addition, some implementation inspirations were taken from paper re-implementation <a href=https://github.com/SeoSangwoo/Attention-Based-BiLSTM-relation-extraction>github repo</a> (data transform, attention details).<br>\n",
    "Honestly, this is quite a raw try (due to time constraints), with practically out-of-the-box model parameters. The model clearly overfits (for the sake of time I've skipped extra L2 regularisation used in repo), but nevertheless it reaches F1 around 0.68 (on the offical test set) practically without parameters adjustments. Although shy of 0.84 reported in the paper for this architecture, with some tweaks and polishes I believe the score would improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialise the libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os.path\n",
    "import gensim\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v2.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0-dev20190821'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Files:\n",
    "#input TEXT files:\n",
    "train_txt_file = '../data/raw/TRAIN_FILE.TXT'\n",
    "test_txt_file = '../data/raw/TEST_FILE.txt'\n",
    "test_file_full = '../data/raw/TEST_FILE_FULL.TXT'\n",
    "\n",
    "#output TXT files:\n",
    "train_out_file = '../output/train_output.txt'\n",
    "test_out_file = '../output/output.txt'\n",
    "\n",
    "#word embeddings file:\n",
    "emb_path='../externals/'\n",
    "emb_vec='cc.en.300.vec.gz'\n",
    "\n",
    "\n",
    "# 2. validation set size:\n",
    "test_size=0.15\n",
    "\n",
    "# 3. Model params:\n",
    "n_hidden_rnn=200\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = np.sqrt(2)\n",
    "rnn_dropout = 0.6\n",
    "out_dropout = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training set into training-validation\n",
    "use standard scikit-learn for that, regardless classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get stuff from data_prep.py helper\n",
    "import data_prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load data and split to train-validation\n",
    "txt, lbl = data_prep.load_data_and_labels(train_txt_file)\n",
    "X_train, X_test, y_train, y_test = train_test_split(txt, lbl, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionaries\n",
    "as simple token-to-index and back dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "#Build custom dictionary with special tokens - PAD\n",
    "special_tokens = ['<PAD>']\n",
    "\n",
    "#build dictionary\n",
    "def build_dict(text, special_tokens):\n",
    "    \"\"\"\n",
    "        text: list of text sentences\n",
    "        special_tokens: padding token\n",
    "    \"\"\"\n",
    "    #Generate tokens from the text\n",
    "    tokens = []\n",
    "    for line in text:\n",
    "        token = line.split()\n",
    "        tokens.append(token)\n",
    "    \n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    \n",
    "    # Create mappings from tokens to indices and vice versa, \n",
    "    # special tokens coming first (index 0)\n",
    "    \n",
    "    idx2tok=special_tokens+list(set([tkn for tkns in tokens\n",
    "                                     for tkn in tkns if tkn not in special_tokens]))\n",
    "\n",
    "    i=0\n",
    "    for tkn in idx2tok:\n",
    "        tok2idx[tkn]=i\n",
    "        i +=1\n",
    "        \n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(X_train+X_test, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup functions for the mapping between tokens and ids for a sentence\n",
    "def words2idxs(tokens_list):\n",
    "    return [token2idx[word] for word in tokens_list]\n",
    "\n",
    "def idxs2words(idxs):\n",
    "    return [idx2token[idx] for idx in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings import\n",
    "use *chakin* here for downloading fasttext vec file first time.<br>\n",
    "TODO: find a way to get binary format (for the speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "emb_file=emb_path+emb_vec\n",
    "\n",
    "#check if the file is already there (download only the first time)\n",
    "if not os.path.exists(emb_file):\n",
    "    import chakin\n",
    "    emb_file = chakin.download(number=2, save_dir=emb_path)\n",
    "    \n",
    "fb_embeddings = KeyedVectors.load_word2vec_format(emb_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19965, 300)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = data_prep.load_pretrained_emb(fb_embeddings, 300, idx2token)\n",
    "words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating batches of batch_size from list of raw (cleaned) sentences, add one-hot labels\n",
    "def batches_generator(batch_size, sentences, labels,\n",
    "                      shuffle=True, allow_smaller_last_batch=True):\n",
    "    \"\"\"Generates padded batches of tokens and one-hot labels\"\"\"\n",
    "    \n",
    "    n_samples = len(sentences)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_sentence = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            x_list.append(words2idxs(sentences[idx].split()))\n",
    "            y_list.append(labels[idx])\n",
    "            max_len_sentence = max(max_len_sentence, len(sentences[idx].split()))\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_sentence], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = y_list\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            lengths[n] = utt_len\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build BiLSTM Model\n",
    "add attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "\n",
    "    # Placeholders for input text and labels.\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
    "    self.input_labels = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_labels')\n",
    "  \n",
    "    # Placeholder for lengths of the sentences.\n",
    "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
    "    \n",
    "    # Placeholder for rnn layers dropout keep probability.\n",
    "    self.rnn_dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    \n",
    "    # Placeholder for an output layer dropout keep probability.\n",
    "    self.out_dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    \n",
    "    # Placeholder for a learning rate (tf.float32).\n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, name='learning_rate_ph')\n",
    "    \n",
    "    \n",
    "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add attention later\n",
    "def attention(inputs):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, num_classes):\n",
    "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
    "    \n",
    "#TODO: create a proper way for selecting random or pre-trained words embeddings\n",
    "#embedding_dim=300 - for pre-trained ones \n",
    "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
    "    #initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    initial_embedding_matrix = data_prep.load_pretrained_emb(fb_embeddings, embedding_dim, idx2token)\n",
    "    embedding_matrix_variable = tf.Variable(initial_embedding_matrix, dtype=tf.float32)\n",
    "    \n",
    "    # Look up embeddings for self.input_batch\n",
    "    # Shape: [batch_size, sentence_len, embedding_dim]\n",
    "    embeddings =  tf.nn.embedding_lookup(embedding_matrix_variable\n",
    "                                         ,ids=self.input_batch)\n",
    "    \n",
    "    # LSTM cells with n_hidden_rnn units, dropout initializing all *_keep_prob with dropout placeholder.\n",
    "    forward_cell =  tf.nn.rnn_cell.DropoutWrapper(\n",
    "                                            tf.nn.rnn_cell.LSTMCell(num_units=n_hidden_rnn)\n",
    "                                           ,input_keep_prob=self.rnn_dropout_ph\n",
    "                                           ,output_keep_prob=self.rnn_dropout_ph\n",
    "                                           ,state_keep_prob=self.rnn_dropout_ph\n",
    "                                           )\n",
    "    backward_cell =  tf.nn.rnn_cell.DropoutWrapper(\n",
    "                                            tf.nn.rnn_cell.LSTMCell(num_units=n_hidden_rnn)\n",
    "                                           ,input_keep_prob=self.rnn_dropout_ph\n",
    "                                           ,output_keep_prob=self.rnn_dropout_ph\n",
    "                                           ,state_keep_prob=self.rnn_dropout_ph\n",
    "                                           )\n",
    "    \n",
    "    # Bidirectional Dynamic RNN\n",
    "    # Shape: [batch_size, sentence_len, 2 * n_hidden_rnn]. \n",
    "    # with self.lengths\n",
    "    (rnn_output_fw, rnn_output_bw), _ =  tf.nn.bidirectional_dynamic_rnn(\n",
    "                                                                          cell_fw=forward_cell\n",
    "                                                                         ,cell_bw=backward_cell\n",
    "                                                                         ,inputs=embeddings\n",
    "                                                                         ,sequence_length=self.lengths\n",
    "                                                                         ,dtype=tf.float32\n",
    "                                                                        )\n",
    "    \n",
    "    #ADDING fw and bw, as in the paper. Try to CONCAT?!\n",
    "    #Shape: [batch_size, sentence_len, 1 * n_hidden_rnn].\n",
    "    rnn_output = tf.add(rnn_output_fw, rnn_output_bw)\n",
    "\n",
    "    #Add Attention\n",
    "    # attn: [batch_size, 1 * n_hidden_rnn], alphas: [batch_size, sentence_len]\n",
    "    attn, alphas = attention(rnn_output)\n",
    "    \n",
    "    #Dropout for Attention layer\n",
    "    h_star = tf.nn.dropout(attn\n",
    "                           ,rate=1 - self.out_dropout_ph)\n",
    "    \n",
    "    # Dense layer on top.\n",
    "    # Shape: [batch_size, num_classes]   \n",
    "    self.logits = tf.layers.dense(attn, num_classes, activation=None)\n",
    "    \n",
    "    \n",
    "BiLSTMModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Attention\n",
    "re-creating the procedure in the paper (with inspirations from official github), then tweak it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following formulas in the paper:\n",
    "def attention(inputs):\n",
    "    #get the w vector\n",
    "    #Shape: [1 * n_hidden_rnn]\n",
    "    hidden_size = inputs.shape[2].value\n",
    "    w = tf.get_variable(\"w\", [hidden_size])\n",
    "    \n",
    "    #get M\n",
    "    m = tf.tanh(inputs)\n",
    "\n",
    "    #get dot product w, m\n",
    "    #Shape: [batch_size, sentence_len]\n",
    "    wm = tf.tensordot(m, w, axes=1, name='wm')\n",
    "    #Shape: [batch_size, sentence_len]\n",
    "    alphas = tf.nn.softmax(wm, name='alphas')\n",
    "\n",
    "    #get output r\n",
    "    #Shape: [batch_size, 1 * n_hidden_rnn]\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    # Final output h-star with tanh\n",
    "    output = tf.tanh(output)\n",
    "\n",
    "    return output, alphas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(self):\n",
    "    #just take argmax along axis 1 -num_classes\n",
    "    self.predictions = tf.argmax(self.logits, axis=1)\n",
    "    \n",
    "    \n",
    "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self):\n",
    "    \n",
    "    loss_tensor =  tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                                            labels=self.input_labels\n",
    "                                                            ,logits=self.logits\n",
    "                                                             )\n",
    "    self.loss = tf.reduce_mean(loss_tensor)\n",
    "    \n",
    "    \n",
    "BiLSTMModel.__compute_loss = classmethod(compute_loss)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(self):\n",
    "    \n",
    "    correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_labels, 1))\n",
    "    \n",
    "    self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "    \n",
    "BiLSTMModel.__compute_accuracy = classmethod(compute_accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
    "    \n",
    "    self.optimizer =  tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)\n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)  \n",
    "    clip_norm = tf.cast(1.0, tf.float32)\n",
    "    self.grads_and_vars =  [(tf.clip_by_norm(grds, clip_norm), vrs) for grds, vrs in self.grads_and_vars]\n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
    "\n",
    "    \n",
    "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, vocabulary_size, embedding_dim, n_hidden_rnn, num_classes):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, num_classes)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss()\n",
    "    self.__compute_accuracy()\n",
    "    self.__perform_optimization()\n",
    "\n",
    "    \n",
    "BiLSTMModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Training and Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, rnn_dropout, out_dropout):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.input_labels: y_batch,\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.rnn_dropout_ph: rnn_dropout,\n",
    "                 self.out_dropout_ph: out_dropout,\n",
    "                 self.lengths: lengths}\n",
    "    \n",
    "    session.run(self.train_op, feed_dict=feed_dict)\n",
    "\n",
    "    \n",
    "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, x_batch, lengths):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.lengths: lengths}\n",
    "    \n",
    "    predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "def eval_model(model, session, sentences, labels):\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch, lengths in batches_generator(1, sentences, labels):\n",
    "        pred_batch = model.predict_for_batch(session, x_batch, lengths)\n",
    "        \n",
    "        y_true.append(np.argmax(y_batch[0]))\n",
    "        y_pred.append(pred_batch[0])\n",
    "        \n",
    "    correct_predictions = np.equal(y_pred, y_true)\n",
    "    accuracy = np.mean(correct_predictions)\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, labels=np.array(range(1, 19)), average=\"macro\")\n",
    "    recall = recall_score(y_true, y_pred, labels=np.array(range(0, 19)), average=\"macro\")\n",
    "    precision = precision_score(y_true, y_pred, labels=np.array(range(0, 19)), average=\"macro\")\n",
    "    \n",
    "    print(\"accuracy:\", accuracy, \"precision:\", precision, \"recall:\", recall, \"F1:\", f1)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN \n",
    "train the model and check validation set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = BiLSTMModel(vocabulary_size=len(idx2token)\n",
    "                    ,num_classes=19\n",
    "                    ,embedding_dim=300\n",
    "                    ,n_hidden_rnn=n_hidden_rnn\n",
    "                    )\n",
    "\n",
    "batch_size = batch_size\n",
    "n_epochs = n_epochs\n",
    "learning_rate = learning_rate\n",
    "learning_rate_decay = learning_rate_decay\n",
    "rnn_dropout = rnn_dropout\n",
    "out_dropout = out_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "-------------------- Epoch 1 of 5 --------------------\n",
      "Train data evaluation:\n",
      "accuracy: 0.010441176470588235 precision: 0.016697997302586438 recall: 0.05415233824539162 F1: 0.002677174447720129\n",
      "Validation data evaluation:\n",
      "accuracy: 0.01 precision: 0.0006043817678166709 recall: 0.048582995951417005 F1: 0.0012602394454946438\n",
      "-------------------- Epoch 2 of 5 --------------------\n",
      "Train data evaluation:\n",
      "accuracy: 0.831764705882353 precision: 0.7945768159572211 recall: 0.7766289129378009 F1: 0.7767107732542663\n",
      "Validation data evaluation:\n",
      "accuracy: 0.6966666666666667 precision: 0.6379640058570133 recall: 0.6420945434291253 F1: 0.6457088913427536\n",
      "-------------------- Epoch 3 of 5 --------------------\n",
      "Train data evaluation:\n",
      "accuracy: 0.9766176470588235 precision: 0.9214321372851381 recall: 0.9321062852445395 F1: 0.9250925232196567\n",
      "Validation data evaluation:\n",
      "accuracy: 0.7266666666666667 precision: 0.6839858864825219 recall: 0.7061958268990166 F1: 0.7034025531704797\n",
      "-------------------- Epoch 4 of 5 --------------------\n",
      "Train data evaluation:\n",
      "accuracy: 0.9977941176470588 precision: 0.9452119074754122 recall: 0.9452909800811422 F1: 0.9423677151791028\n",
      "Validation data evaluation:\n",
      "accuracy: 0.735 precision: 0.735272027749947 recall: 0.7041711991429079 F1: 0.7271132576763987\n",
      "-------------------- Epoch 5 of 5 --------------------\n",
      "Train data evaluation:\n",
      "accuracy: 0.9995588235294117 precision: 0.9469866784160921 recall: 0.9470586213191239 F1: 0.9441022280429087\n",
      "Validation data evaluation:\n",
      "accuracy: 0.7191666666666666 precision: 0.7277645607295334 recall: 0.6837961945756039 F1: 0.7135685841956068\n",
      "...training finished.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    print('Train data evaluation:')\n",
    "    eval_model(model, sess, X_train, y_train)\n",
    "    print('Validation data evaluation:')\n",
    "    eval_model(model, sess, X_test, y_test)\n",
    "    \n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size, X_train, y_train):\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, rnn_dropout, out_dropout)\n",
    "        \n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7258333333333333 precision: 0.7119892918389158 recall: 0.7043446774697099 F1: 0.7163152982754161\n"
     ]
    }
   ],
   "source": [
    "#Again evaluate on the validation set:\n",
    "eval_model(model, sess, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "predict the relationships and store in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_relationship(model, session, sentences, f_id, out_file):\n",
    "    \n",
    "    print(\"Running predictions\")\n",
    "    y_pred = []\n",
    "    dummies = np.zeros(len(sentences), dtype=np.int32)\n",
    "    for x_batch, _, lengths in batches_generator(1, sentences, dummies, shuffle=False):\n",
    "        pred_batch = model.predict_for_batch(session, x_batch, lengths)\n",
    "        \n",
    "        y_pred.append(pred_batch[0])\n",
    "        \n",
    "    print(\"Writing to file\")\n",
    "    \n",
    "    with open(out_file, 'w') as f:\n",
    "        for i in range(len(y_pred)):\n",
    "            f.write(\"{}\\t{}\\n\".format(i+f_id, data_prep.label2class[y_pred[i]]))\n",
    "            \n",
    "    print(\"Done!\")    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717 8001\n"
     ]
    }
   ],
   "source": [
    "test_txt, f_id = data_prep.load_test_data(test_txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions\n",
      "Writing to file\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "predict_relationship(model, sess, test_txt, f_id, test_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_f, lbl_f = data_prep.load_data_and_labels(test_file_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7004048582995951 precision: 0.6643133394485279 recall: 0.6727478440415117 F1: 0.680718751050499\n"
     ]
    }
   ],
   "source": [
    "eval_model(model, sess, txt_f, lbl_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
